{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-de1446a38b4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_accuracy(all_gt, all_label):\n",
    "    return len(np.argwhere(all_gt==all_label))/float(len(all_gt))\n",
    "\n",
    "def get_sensitivity(all_gt, all_label):\n",
    "    loc = np.where(all_gt==1)\n",
    "    return len(np.argwhere(all_gt[loc]==all_label[loc]))/float(len(all_gt[loc]))\n",
    "\n",
    "def get_specificity(all_gt, all_label):\n",
    "    loc = np.where(all_gt==0)\n",
    "    return len(np.argwhere(all_gt[loc]==all_label[loc]))/float(len(all_gt[loc]))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "def get_auc(y_true,y_pred):\n",
    "    n_bootstraps = 1000\n",
    "    bootstrapped_scores = []\n",
    "    np.random.seed(seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = np.random.choice(range(0, len(y_pred)), len(y_pred), replace=True)\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        score = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "    confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "    return roc_auc_score(y_true, y_pred), confidence_lower, confidence_upper\n",
    "\n",
    "def hyper_parameter_search(train_feat, train_1p19q, val_feat, val_1p19q, C_array):\n",
    "    result = {'accuracy': [], 'AUC': [], 'sensitivity': [], 'specificity': [], 'C_val': C_array}\n",
    "\n",
    "    for C_val in C_array:\n",
    "\n",
    "        logreg_l1 = LogisticRegression(C=C_val, penalty = 'l1', tol=0.01)\n",
    "        logreg_l2 = LogisticRegression(C=C_val, penalty = 'l2', tol=0.01)\n",
    "        \n",
    "        logreg_l1.fit(train_sig_imaging, train_1p19q)\n",
    "        logreg_l2.fit(train_sig_imaging, train_1p19q)\n",
    "\n",
    "        Z_l1 = logreg_l1.predict_proba(val_sig_imaging)\n",
    "        Z_l2 = logreg_l2.predict_proba(val_sig_imaging)\n",
    "\n",
    "        # accuracy\n",
    "        auc_l1 = get_auc(val_1p19q, Z_l1[:,1])[0]\n",
    "        auc_l2 = get_auc(val_1p19q, Z_l2[:,1])[0]\n",
    "        # AUC\n",
    "        acc_l1 = get_accuracy(val_1p19q, np.round(Z_l1[:,1]))\n",
    "        acc_l2 = get_accuracy(val_1p19q, np.round(Z_l1[:,1]))\n",
    "        # sensitivity\n",
    "        sen_l1 = get_sensitivity(val_1p19q, np.round(Z_l1[:,1]))\n",
    "        sen_l2 = get_sensitivity(val_1p19q, np.round(Z_l1[:,1]))\n",
    "        # specificity\n",
    "        spe_l1 = get_specificity(val_1p19q, np.round(Z_l1[:,1]))\n",
    "        spe_l2 = get_specificity(val_1p19q, np.round(Z_l1[:,1]))\n",
    "\n",
    "        result['AUC'].append([auc_l1, auc_l2])\n",
    "        result['accuracy'].append([acc_l1, acc_l2])\n",
    "        result['sensitivity'].append([sen_l1, sen_l2])\n",
    "        result['specificity'].append([spe_l2, spe_l2])\n",
    "\n",
    "    # Now, obtain the best C-val with best AUC and Accuracy combo\n",
    "    AUC_list = result['AUC']\n",
    "    accuracy_list = result['accuracy']\n",
    "\n",
    "    summed = np.add(AUC_list, accuracy_list)\n",
    "\n",
    "    row,col = np.where(summed==summed.max())\n",
    "\n",
    "    if col[0] == 0:\n",
    "        best_penalty_type = 'l1'\n",
    "    else:\n",
    "        best_penalty_type = 'l2'\n",
    "\n",
    "    best_c = C_array[row][0]\n",
    "\n",
    "\n",
    "    return best_penalty_type, best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify which GPU to be used\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "seed = 0\n",
    "\n",
    "# load train data\n",
    "os.chdir('/rsrch1/bcb/Imaging-Genomics_SIBL/DONNIE_KIM/Brachy_deep_learning/IDH_Prediction/data/data_splitted_balanced_20/train')\n",
    "train_FLAIR = np.load('train_FLAIR.npy')\n",
    "train_T2 = np.load('train_T2.npy')\n",
    "train_T1 = np.load('train_T1.npy')\n",
    "train_T1post = np.load('train_T1post.npy')\n",
    "\n",
    "train_1p19q = np.load('train_1p19q.npy')\n",
    "train_1p19q = train_1p19q.astype(np.float32).reshape(-1,1)\n",
    "train_age = np.expand_dims(np.load('train_age.npy'),1)\n",
    "train_KPS = np.expand_dims(np.load('train_KPS.npy'),1)\n",
    "train_gender = np.expand_dims(np.load('train_gender.npy'),1)\n",
    "\n",
    "# load val data\n",
    "os.chdir('/rsrch1/bcb/Imaging-Genomics_SIBL/DONNIE_KIM/Brachy_deep_learning/IDH_Prediction/data/data_splitted_balanced_20/val')\n",
    "val_FLAIR = np.load('val_FLAIR.npy')\n",
    "val_T2 = np.load('val_T2.npy')\n",
    "val_T1 = np.load('val_T1.npy')\n",
    "val_T1post = np.load('val_T1post.npy')\n",
    "\n",
    "val_1p19q = np.load('val_1p19q.npy')\n",
    "val_1p19q = val_1p19q.astype(np.float32).reshape(-1,1)\n",
    "val_age = np.expand_dims(np.load('val_age.npy'),1)\n",
    "val_KPS = np.expand_dims(np.load('val_KPS.npy'),1)\n",
    "val_gender = np.expand_dims(np.load('val_gender.npy'),1)\n",
    "\n",
    "#load saved models\n",
    "os.chdir('/rsrch1/bcb/Imaging-Genomics_SIBL/DONNIE_KIM/Brachy_deep_learning/IDH_Prediction/data/data_splitted_balanced_20/outputs/models/')\n",
    "model_FLAIR = load_model('flair_model_conv2.h5')\n",
    "model_T2 = load_model('T2_model_conv2.h5')\n",
    "model_T1 = load_model('T1_model_conv2.h5')\n",
    "model_T1post = load_model('T1post_model_conv2.h5')\n",
    "\n",
    "# obtain sigmoid probs for train set\n",
    "train_sig_FLAIR = model_FLAIR.predict(train_FLAIR,batch_size=16)\n",
    "train_sig_T2 = model_T2.predict(train_T2,batch_size=16)\n",
    "train_sig_T1 = model_T1.predict(train_T1,batch_size=16)\n",
    "train_sig_T1post = model_T1post.predict(train_T1post,batch_size=16)\n",
    "\n",
    "# obtain sigmoid probs for val set\n",
    "val_sig_FLAIR = model_FLAIR.predict(val_FLAIR,batch_size=16)\n",
    "val_sig_T2 = model_T2.predict(val_T2,batch_size=16)\n",
    "val_sig_T1 = model_T1.predict(val_T1,batch_size=16)\n",
    "val_sig_T1post = model_T1post.predict(val_T1post,batch_size=16)\n",
    "\n",
    "# hyperparameter for logistic: C value (i.e. regularization strength)\n",
    "C_array = np.logspace(-2,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case1: Consider only the image sequences\n",
    "train_imaging = np.hstack((train_sig_FLAIR,train_sig_T2,train_sig_T1,train_sig_T1post))\n",
    "val_imaging = np.hstack((val_sig_FLAIR,val_sig_T2,val_sig_T1,val_sig_T1post))\n",
    "\n",
    "best_penalty_type_imaging, best_c_imaging = hyper_parameter_search(train_feat= train_imaging, train_1p19q= train_1p19q, val_feat=val_imaging, val_1p19q=val_1p19q, C_array= C_array)\n",
    "\n",
    "# Case2: Consider imaging + age\n",
    "train_imaging_age = np.hstack((train_sig_FLAIR,train_sig_T2,train_sig_T1,train_sig_T1post, train_age))\n",
    "val_imaging_age = np.hstack((val_sig_FLAIR,val_sig_T2,val_sig_T1,val_sig_T1post, val_age))\n",
    "\n",
    "best_penalty_type_imaging_age, best_c_imaging_age = hyper_parameter_search(train_feat= train_imaging_age, train_1p19q= train_1p19q, val_feat=val_imaging_age, val_1p19q=val_1p19q, C_array= C_array)\n",
    "\n",
    "# Case3: Consider imaging + age + KPS + gender\n",
    "train_all = np.hstack((train_sig_FLAIR,train_sig_T2,train_sig_T1,train_sig_T1post, train_age, train_KPS, train_gender))\n",
    "val_all = np.hstack((val_sig_FLAIR,val_sig_T2,val_sig_T1,val_sig_T1post, val_age, val_KPS, val_gender))\n",
    "\n",
    "best_penalty_type_all, best_c_all = hyper_parameter_search(train_feat= train_all, train_1p19q= train_1p19q, val_feat=val_all, val_1p19q=val_1p19q, C_array= C_array)\n",
    "\n",
    "#combine val and training features\n",
    "train_val_imaging = np.vstack((train_imaging,val_imaging))\n",
    "train_val_imaging_age = np.vstack((train_imaging_age,val_imaging_age))\n",
    "train_val_all = np.vstack((train_imaging_all,val_imaging_all))\n",
    "\n",
    "train_val_1p19q = np.append(train_1p19q, val_1p19q)\n",
    "\n",
    "# train with best reg strength and penalty term\n",
    "logreg_imaging = LogisticRegression(C=best_c_imaging, penalty = best_penalty_type_imaging, tol=0.01)\n",
    "logreg_imaging_age = LogisticRegression(C=best_c_imaging_age, penalty = best_penalty_type_imaging_age, tol=0.01)\n",
    "logreg_all = LogisticRegression(C=best_c_all, penalty = best_penalty_type_all, tol=0.01)\n",
    "\n",
    "logreg_imaging.fit(train_val_imaging, train_val_1p19q)\n",
    "logreg_imaging_age.fit(train_val_imaging_age, train_val_1p19q)\n",
    "logreg_all.fit(train_val_all, train_val_1p19q)\n",
    "\n",
    "# save the trained logistic regression classifiers\n",
    "np.save('logreg_imaging.npy', logreg_imaging.__dict__)\n",
    "np.save('logreg_imaging_age.npy', logreg_imaging_age.__dict__)\n",
    "np.save('logreg_all.npy', logreg_all.__dict__)\n",
    "\n",
    "# load test data\n",
    "os.chdir('/rsrch1/bcb/Imaging-Genomics_SIBL/DONNIE_KIM/Brachy_deep_learning/IDH_Prediction/data/data_splitted_balanced_20/test')\n",
    "test_FLAIR = np.load('test_FLAIR.npy')\n",
    "test_T2 = np.load('test_T2.npy')\n",
    "test_T1 = np.load('test_T1.npy')\n",
    "test_T1post = np.load('test_T1post.npy')\n",
    "\n",
    "test_1p19q = np.load('test_1p19q.npy')\n",
    "test_age = np.expand_dims(np.load('test_age.npy'),1)\n",
    "test_KPS = np.expand_dims(np.load('test_KPS.npy'),1)\n",
    "test_gender = np.expand_dims(np.load('test_gender.npy'),1)\n",
    "\n",
    "# obtain sigmoid probs for test set\n",
    "test_sig_FLAIR = model_FLAIR.predict(test_FLAIR,batch_size=16)\n",
    "test_sig_T2 = model_T2.predict(test_T2,batch_size=16)\n",
    "test_sig_T1 = model_T1.predict(test_T1,batch_size=16)\n",
    "test_sig_T1post = model_T1post.predict(test_T1post,batch_size=16)\n",
    "\n",
    "test_imaging = np.hstack((test_sig_FLAIR,test_sig_T2, test_sig_T1, test_sig_T1post))\n",
    "test_imaging_age = np.hstack((test_sig_FLAIR,test_sig_T2, test_sig_T1, test_sig_T1post, test_age))\n",
    "test_all = np.hstack((test_sig_FLAIR,test_sig_T2, test_sig_T1, test_sig_T1post, test_age, test_KPS, test_gender))\n",
    "\n",
    "Z_imaging = logreg_imaging.predict_proba(test_imaging)\n",
    "Z_imaging_age = logreg_imaging_age.predict_proba(test_imaging_age)\n",
    "Z_all = logreg_all.predict_proba(test_all)\n",
    "\n",
    "imaging_acc = get_accuracy(test_1p19q,np.round(Z_imaging[:,1]))\n",
    "imaging_sen = get_sensitivity(test_1p19q,np.round(Z_imaging[:,1]))\n",
    "imaging_spe = get_specificity(test_1p19q,np.round(Z_imaging[:,1]))\n",
    "get_auc(test_1p19q,Z_imaging[:,1])\n",
    "\n",
    "imaging_age_acc = get_accuracy(test_1p19q,np.round(Z_imaging_age[:,1]))\n",
    "imaging_age_sen = get_sensitivity(test_1p19q,np.round(Z_imaging_age[:,1]))\n",
    "imaging_age_spe = get_specificity(test_1p19q,np.round(Z_imaging_age[:,1]))\n",
    "get_auc(test_1p19q,Z_imaging_age[:,1])\n",
    "\n",
    "all_acc = get_accuracy(test_1p19q,np.round(Z_all[:,1]))\n",
    "all_sen = get_sensitivity(test_1p19q,np.round(Z_all[:,1]))\n",
    "all_spe = get_specificity(test_1p19q,np.round(Z_all[:,1]))\n",
    "get_auc(test_1p19q,Z_all[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
